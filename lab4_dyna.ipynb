{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vap326/cse337/blob/main/lab4_dyna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 4: TD and Dyna\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3O73YRqcSpVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Implement SARSA with n-step TD (n=5) on CliffWalking\n",
        "\n",
        "**Objective:**  \n",
        "In this exercise, you will implement the **SARSA algorithm** using **n-step temporal-difference learning with n=5**. You will apply your implementation to the **CliffWalking environment** in Gymnasium, and analyze how multi-step returns influence learning compared to standard 1-step SARSA.\n",
        "\n",
        "---\n",
        "\n",
        "### Environment\n",
        "- Use `CliffWalking-v1`\n",
        "\n",
        "---\n",
        "\n",
        "### Instructions\n",
        "1. Implement **SARSA with n-step TD updates (n=5)**:\n",
        "   - Maintain an action-value table \\(Q(s,a)\\).\n",
        "   - Use ε-greedy exploration.\n",
        "   - Store states, actions, and rewards for the last 5 steps.\n",
        "   - After each step, compute the n-step return: G_t\n",
        "   - Update \\(Q(s_t,a_t)\\) toward \\(G_t\\).\n",
        "\n",
        "2. Train your agent for several thousand episodes (e.g., 5,000).\n",
        "\n",
        "3. Plot the **episode rewards over time** to visualize learning progress.\n",
        "\n",
        "4. Compare qualitatively with 1-step SARSA:\n",
        "   - Does n-step SARSA converge faster or slower?\n",
        "   - How do the policies differ near the cliff?\n",
        "\n",
        "---\n",
        "\n",
        "### Deliverables\n",
        "- Python code implementing SARSA with TD(5) (notebook in Github).  \n",
        "- A plot of episode number vs episode return (plot in a cell below).  \n",
        "- A short discussion (1 paragraph) comparing the results with standard SARSA.  \n"
      ],
      "metadata": {
        "id": "mnJD-ntoxjeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ORsNHBnkSbyS",
        "outputId": "13128359-bf1d-4aca-9dcc-e251e312fca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-281377853.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# Convert buffers to lists for safe indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mstates_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mactions_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mrewards_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Starter code for Exercise (you can use this code, or extend your code from previous lab)\n",
        "Implement SARSA with TD(5) on CliffWalking-v1\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Environment\n",
        "env = gym.make(\"CliffWalking-v1\")\n",
        "\n",
        "# Parameters\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "alpha = 0.1           # step size (learning rate)\n",
        "gamma = 0.99          # discount factor\n",
        "epsilon = 0.1         # epsilon for epsilon-greedy policy\n",
        "n_step = 5           # number of steps for TD(n)\n",
        "n_episodes = 5000\n",
        "\n",
        "#maintain a buffer for all the transitions\n",
        "# Why buffer for TDs?\n",
        "# → Maintaining a buffer of the last n states, actions, and rewards\n",
        "#   allows us to compute n-step returns efficiently by looking back\n",
        "#   at recent trajectories instead of waiting until the episode ends.\n",
        "\n",
        "# Initialize Q-table\n",
        "Q = np.zeros((n_states, n_actions))\n",
        "\n",
        "def epsilon_greedy(state):\n",
        "    \"\"\"Choose an action using epsilon-greedy policy.\"\"\"\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_actions)\n",
        "    return np.argmax(Q[state])\n",
        "\n",
        "# Track returns\n",
        "episode_returns = []\n",
        "\n",
        "for ep in range(n_episodes):\n",
        "    state, _ = env.reset()\n",
        "    action = epsilon_greedy(state)\n",
        "\n",
        "    # Buffers to store the trajectory\n",
        "    states = deque()\n",
        "    actions = deque()\n",
        "    rewards = deque()\n",
        "\n",
        "    T = float(\"inf\")\n",
        "    t = 0\n",
        "    G = 0\n",
        "    done = False\n",
        "\n",
        "    while True:\n",
        "        if t < T:\n",
        "            # Take real step in the environment\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            if done:\n",
        "                T = t + 1\n",
        "            else:\n",
        "                next_action = epsilon_greedy(next_state)\n",
        "                state = next_state\n",
        "                action = next_action\n",
        "\n",
        "        # Time index for state/action to update\n",
        "        tau = t - n_step + 1\n",
        "        if tau >= 0:\n",
        "            # TODO: Compute the n-step return G for state tau\n",
        "            # Hint: use rewards[tau : tau+n] plus Q(s_t+n, a_t+n) if not terminal\n",
        "            # Convert buffers to lists for safe indexing\n",
        "            states_list = list(states)\n",
        "            actions_list = list(actions)\n",
        "            rewards_list = list(rewards)\n",
        "\n",
        "            # Compute the n-step return\n",
        "            G = 0.0\n",
        "            for i in range(tau, min(tau + n_step, T)):\n",
        "                G += (gamma ** (i - tau)) * rewards_list[i]\n",
        "\n",
        "            if tau + n_step < T and (tau + n_step) < len(states_list):\n",
        "                s_tau_n = states_list[tau + n_step]\n",
        "                a_tau_n = actions_list[tau + n_step]\n",
        "                G += (gamma ** n_step) * Q[s_tau_n, a_tau_n]\n",
        "\n",
        "            # Update Q\n",
        "            s_tau = states_list[tau]\n",
        "            a_tau = actions_list[tau]\n",
        "            Q[s_tau, a_tau] += alpha * (G - Q[s_tau, a_tau])\n",
        "\n",
        "        if tau == T - 1:\n",
        "            break\n",
        "\n",
        "        t += 1\n",
        "\n",
        "    episode_returns.append(sum(rewards))\n",
        "\n",
        "# Plot learning curve\n",
        "plt.plot(episode_returns)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Return\")\n",
        "plt.title(\"SARSA with TD(5) on CliffWalking\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Dyna-Q for CliffWalking\n",
        "\n",
        "**Objective**  \n",
        "Implement **Dyna-Q** on **CliffWalking-v1** and compare its learning performance to **SARSA (1-step)** and **SARSA TD(5)**. You will analyze sample efficiency, stability near the cliff, and sensitivity to planning steps.\n",
        "\n",
        "---\n",
        "\n",
        "### Environment\n",
        "- Use `CliffWalking-v1`\n",
        "---\n",
        "\n",
        "### Part A — Dyna-Q (Implementation)\n",
        "1. **Q-table**: maintain `Q[s, a]` (tabular).\n",
        "2. **Model**: learn an empirical model from experience.\n",
        "   - For each observed transition `(s, a, r, s')`, update a dictionary:\n",
        "     - Minimal: store the most recent `(s', r)` for `(s, a)`, **or**\n",
        "     - Advanced: store a **multiset** of outcomes for `(s, a)` with counts (to sample stochastically).\n",
        "3. **Real update (Q-learning)** after each env step:\n",
        "   Q(s,a) ← Q(s,a) + α * (r + γ * max_a' Q(s',a') - Q(s,a))\n",
        "4. **Planning updates**: after each real step, perform `N` simulated updates:\n",
        "   - Sample a previously seen `(s_p, a_p)` from the model.\n",
        "   - Sample `(r_p, s'_p)` from that entry.\n",
        "   - Apply the same Q-learning backup using `(s_p, a_p, r_p, s'_p)`.\n",
        "5. Use epsilon-greedy exploration.\n",
        "\n",
        "---\n",
        "\n",
        "### Part B — Baselines (Re-use / Implement)\n",
        "- **SARSA (1-step)** with ε-greedy:\n",
        "  \\[\n",
        "  Q(s,a) \\leftarrow Q(s,a) + \\alpha\\big[r + \\gamma Q(s',a') - Q(s,a)\\big]\n",
        "  \\]\n",
        "- **SARSA TD(5)** (n-step SARSA with \\(n=5\\)), as in Exercise 1.\n",
        "\n",
        "Use the **same** γ, α, ε schedule, and number of episodes for a fair comparison.\n",
        "\n",
        "---\n",
        "\n",
        "### Part C — Experiments & Comparisons\n",
        "1. **Learning curves**: plot **episode index vs. episode return** for:\n",
        "   - Dyna-Q with \\(N \\in \\{5, 20, 50\\}\\)\n",
        "   - SARSA (1-step)\n",
        "   - SARSA TD(5)\n",
        "2. **Sample efficiency**: report the **episode number** at which the average return over a sliding window (e.g., 100 episodes) first exceeds a chosen threshold (e.g., −30).\n",
        "3. **Stability near the cliff**: qualitatively inspect trajectories/policies; does the method hug the cliff or leave a safer margin?\n",
        "4. **Sensitivity to planning steps**: compare Dyna-Q across N; discuss diminishing returns vs. computation.\n",
        "5. **Statistical robustness**: run **≥5 seeds**; plot mean ± std (shaded) or report mean ± std of final returns.\n",
        "\n",
        "---\n",
        "\n",
        "### Deliverables\n",
        "- **Code**: A driver script/notebook that reproduces your plots\n",
        "- **Plots** (embedded in the notebook):\n",
        "  - Learning curves (mean ± std across seeds)\n",
        "  - Optional: heatmap of greedy policy/actions on the grid\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h8NKZuvP5GZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dyna-Q vs SARSA(1) vs SARSA TD(5) on CliffWalking-v1\n",
        "# Run in a Jupyter cell. Requires: gymnasium, numpy, matplotlib\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque, defaultdict\n",
        "import random\n",
        "from copy import deepcopy\n",
        "\n",
        "# -------- Hyperparameters and experiment config --------\n",
        "gamma = 0.99\n",
        "alpha = 0.1\n",
        "epsilon = 0.1\n",
        "\n",
        "n_episodes = 2000           # number of episodes per run (adjustable)\n",
        "seeds = [0, 1, 2, 3, 4]     # run >=5 seeds for statistical robustness\n",
        "window = 100                # sliding window for sample-efficiency threshold\n",
        "threshold = -30             # example threshold for average return (CliffWalking returns are negative)\n",
        "\n",
        "# Dyna planning steps to compare\n",
        "dyna_N_list = [5, 20, 50]\n",
        "\n",
        "# For reproducibility across gym resets we set seed per env when creating it in each run\n",
        "\n",
        "# -------- Helper functions --------\n",
        "def epsilon_greedy(Q, state, n_actions, eps):\n",
        "    if np.random.rand() < eps:\n",
        "        return np.random.randint(n_actions)\n",
        "    return int(np.argmax(Q[state]))\n",
        "\n",
        "def run_sarsa_1(env_name, seed, n_episodes, alpha, gamma, epsilon):\n",
        "    env = gym.make(env_name)\n",
        "    env.reset(seed=seed)\n",
        "    n_states = env.observation_space.n\n",
        "    n_actions = env.action_space.n\n",
        "    Q = np.zeros((n_states, n_actions))\n",
        "    returns = []\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        state, _ = env.reset(seed=(seed + ep))  # vary seed slightly per episode to vary randomness\n",
        "        a = epsilon_greedy(Q, state, n_actions, epsilon)\n",
        "        ep_return = 0\n",
        "        done = False\n",
        "        while True:\n",
        "            next_state, reward, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            ep_return += reward\n",
        "\n",
        "            if done:\n",
        "                target = reward\n",
        "                Q[state, a] += alpha * (target - Q[state, a])\n",
        "                break\n",
        "            else:\n",
        "                next_a = epsilon_greedy(Q, next_state, n_actions, epsilon)\n",
        "                target = reward + gamma * Q[next_state, next_a]\n",
        "                Q[state, a] += alpha * (target - Q[state, a])\n",
        "\n",
        "                state = next_state\n",
        "                a = next_a\n",
        "\n",
        "        returns.append(ep_return)\n",
        "\n",
        "    env.close()\n",
        "    return np.array(returns), Q\n",
        "\n",
        "def run_sarsa_n(env_name, seed, n_episodes, alpha, gamma, epsilon, n_step=5):\n",
        "    env = gym.make(env_name)\n",
        "    env.reset(seed=seed)\n",
        "    n_states = env.observation_space.n\n",
        "    n_actions = env.action_space.n\n",
        "    Q = np.zeros((n_states, n_actions))\n",
        "    returns = []\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        state, _ = env.reset(seed=(seed + ep))\n",
        "        action = epsilon_greedy(Q, state, n_actions, epsilon)\n",
        "\n",
        "        states = deque()\n",
        "        actions = deque()\n",
        "        rewards = deque()\n",
        "\n",
        "        T = float(\"inf\")\n",
        "        t = 0\n",
        "        ep_return = 0\n",
        "\n",
        "        while True:\n",
        "            if t < T:\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "                ep_return += reward\n",
        "\n",
        "                states.append(state)\n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "\n",
        "                if done:\n",
        "                    T = t + 1\n",
        "                else:\n",
        "                    next_action = epsilon_greedy(Q, next_state, n_actions, epsilon)\n",
        "                    state = next_state\n",
        "                    action = next_action\n",
        "\n",
        "            tau = t - n_step + 1\n",
        "            if tau >= 0:\n",
        "                # snapshot lists for safe indexing\n",
        "                s_list = list(states)\n",
        "                a_list = list(actions)\n",
        "                r_list = list(rewards)\n",
        "\n",
        "                # compute n-step return\n",
        "                G = 0.0\n",
        "                end = min(tau + n_step, T)   # ensure we don’t pass inf to range()\n",
        "                for i in range(tau, min(tau + n_step, int(T))):\n",
        "                    G += (gamma ** (i - tau)) * r_list[i]\n",
        "                if tau + n_step < T:\n",
        "                    # ensure index exists\n",
        "                    idx = tau + n_step\n",
        "                    if idx < len(s_list):\n",
        "                        s_tau_n = s_list[idx]\n",
        "                        a_tau_n = a_list[idx]\n",
        "                        G += (gamma ** n_step) * Q[s_tau_n, a_tau_n]\n",
        "\n",
        "                s_tau = s_list[tau]\n",
        "                a_tau = a_list[tau]\n",
        "                Q[s_tau, a_tau] += alpha * (G - Q[s_tau, a_tau])\n",
        "\n",
        "                # pop left when we've updated the earliest stored element to avoid unbounded growth\n",
        "                # but only pop if queue length > n_step to keep indexing consistent; we can pop once tau >= 0 and tau < len\n",
        "                # A simpler safe approach: pop left now because we've already used s_tau (the earliest stored)\n",
        "                states.popleft()\n",
        "                actions.popleft()\n",
        "                rewards.popleft()\n",
        "\n",
        "            if tau == T - 1:\n",
        "                break\n",
        "\n",
        "            t += 1\n",
        "\n",
        "        returns.append(ep_return)\n",
        "\n",
        "    env.close()\n",
        "    return np.array(returns), Q\n",
        "\n",
        "def run_dyna_q(env_name, seed, n_episodes, alpha, gamma, epsilon, planning_steps=5, model_type=\"multi\"):\n",
        "    \"\"\"\n",
        "    model_type:\n",
        "      - \"one-step\": model[(s,a)] = (r, s')  most recent (deterministic)\n",
        "      - \"multi\": model[(s,a)] = list of (r, s') outcomes (sample uniformly)\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name)\n",
        "    env.reset(seed=seed)\n",
        "    n_states = env.observation_space.n\n",
        "    n_actions = env.action_space.n\n",
        "    Q = np.zeros((n_states, n_actions))\n",
        "    returns = []\n",
        "\n",
        "    # model: dict from (s,a) -> list of (r,s')\n",
        "    model = defaultdict(list)\n",
        "    seen_sa = set()\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        state, _ = env.reset(seed=(seed + ep))\n",
        "        ep_return = 0\n",
        "        done = False\n",
        "\n",
        "        while True:\n",
        "            a = epsilon_greedy(Q, state, n_actions, epsilon)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            ep_return += reward\n",
        "\n",
        "            # Q-learning real update\n",
        "            if done:\n",
        "                target = reward\n",
        "            else:\n",
        "                target = reward + gamma * np.max(Q[next_state])\n",
        "            Q[state, a] += alpha * (target - Q[state, a])\n",
        "\n",
        "            # update model\n",
        "            if model_type == \"one-step\":\n",
        "                model[(state, a)] = [(reward, next_state)]\n",
        "            else:\n",
        "                model[(state, a)].append((reward, next_state))\n",
        "            seen_sa.add((state, a))\n",
        "\n",
        "            # planning updates\n",
        "            for _ in range(planning_steps):\n",
        "                if len(seen_sa) == 0:\n",
        "                    break\n",
        "                s_p, a_p = random.choice(list(seen_sa))\n",
        "                outcomes = model[(s_p, a_p)]\n",
        "                r_p, s_next_p = random.choice(outcomes)\n",
        "                # Q-learning backup on simulated experience\n",
        "                if s_next_p is None:\n",
        "                    target_p = r_p\n",
        "                else:\n",
        "                    target_p = r_p + gamma * np.max(Q[s_next_p])\n",
        "                Q[s_p, a_p] += alpha * (target_p - Q[s_p, a_p])\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        returns.append(ep_return)\n",
        "\n",
        "    env.close()\n",
        "    return np.array(returns), Q\n",
        "\n",
        "# -------- Run experiments (multiple seeds) --------\n",
        "env_name = \"CliffWalking-v1\"\n",
        "\n",
        "# containers to hold all runs\n",
        "results = {}   # key -> list of returns arrays (one per seed)\n",
        "policies = {}  # store final Q for last seed for quick policy inspection (optional)\n",
        "\n",
        "# Baseline SARSA(1)\n",
        "print(\"Running SARSA(1) across seeds...\")\n",
        "results['SARSA_1'] = []\n",
        "for seed in seeds:\n",
        "    r, Qfinal = run_sarsa_1(env_name, seed, n_episodes, alpha, gamma, epsilon)\n",
        "    results['SARSA_1'].append(r)\n",
        "    policies.setdefault('SARSA_1', []).append(Qfinal)\n",
        "\n",
        "# SARSA TD(5)\n",
        "print(\"Running SARSA TD(5) across seeds...\")\n",
        "results['SARSA_TD5'] = []\n",
        "for seed in seeds:\n",
        "    r, Qfinal = run_sarsa_n(env_name, seed, n_episodes, alpha, gamma, epsilon, n_step=5)\n",
        "    results['SARSA_TD5'].append(r)\n",
        "    policies.setdefault('SARSA_TD5', []).append(Qfinal)\n",
        "\n",
        "# Dyna-Q with various planning steps\n",
        "for N in dyna_N_list:\n",
        "    key = f\"DynaQ_N{N}\"\n",
        "    print(f\"Running {key} across seeds...\")\n",
        "    results[key] = []\n",
        "    for seed in seeds:\n",
        "        r, Qfinal = run_dyna_q(env_name, seed, n_episodes, alpha, gamma, epsilon, planning_steps=N, model_type=\"multi\")\n",
        "        results[key].append(r)\n",
        "        policies.setdefault(key, []).append(Qfinal)\n",
        "\n",
        "# -------- Aggregate statistics across seeds --------\n",
        "def aggregate_returns(list_of_arrays):\n",
        "    # list_of_arrays: list of (n_episodes,) arrays\n",
        "    arr = np.vstack(list_of_arrays)  # shape (n_seeds, n_episodes)\n",
        "    mean = arr.mean(axis=0)\n",
        "    std = arr.std(axis=0)\n",
        "    return mean, std\n",
        "\n",
        "aggregated = {}\n",
        "for k, runs in results.items():\n",
        "    mean, std = aggregate_returns(runs)\n",
        "    aggregated[k] = (mean, std)\n",
        "\n",
        "# -------- Sample efficiency: find first episode where sliding average (window) > threshold --------\n",
        "def first_episode_exceeding_threshold(runs, window, threshold):\n",
        "    # runs: list of arrays (each array length n_episodes)\n",
        "    first_eps = []\n",
        "    for run in runs:\n",
        "        # compute sliding averages\n",
        "        if len(run) < window:\n",
        "            first_eps.append(None)\n",
        "            continue\n",
        "        movavg = np.convolve(run, np.ones(window)/window, mode='valid')  # length n_episodes - window +1\n",
        "        idxs = np.where(movavg > threshold)[0]\n",
        "        if len(idxs) == 0:\n",
        "            first_eps.append(None)\n",
        "        else:\n",
        "            # convert index in movavg to episode number (we choose the *first* episode of the window that meets criterion)\n",
        "            first_eps.append(int(idxs[0] + window - 1))\n",
        "    # return mean ± std excluding None\n",
        "    val_list = [v for v in first_eps if v is not None]\n",
        "    if len(val_list) == 0:\n",
        "        return None, None, first_eps\n",
        "    return np.mean(val_list), np.std(val_list), first_eps\n",
        "\n",
        "sample_efficiency = {}\n",
        "for k, runs in results.items():\n",
        "    mean_ep, std_ep, all_first = first_episode_exceeding_threshold(runs, window, threshold)\n",
        "    sample_efficiency[k] = {'mean_first_ep': mean_ep, 'std_first_ep': std_ep, 'all_first': all_first}\n",
        "\n",
        "# -------- Plot learning curves (mean ± std) --------\n",
        "plt.figure(figsize=(12, 8))\n",
        "for k, (mean, std) in aggregated.items():\n",
        "    episodes = np.arange(1, len(mean)+1)\n",
        "    plt.plot(episodes, mean, label=k)\n",
        "    plt.fill_between(episodes, mean-std, mean+std, alpha=0.2)\n",
        "\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Return (episode total reward)\")\n",
        "plt.title(\"Learning curves: Dyna-Q (various N) vs SARSA(1) vs SARSA TD(5)\\nMean ± Std over seeds\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# -------- Print sample-efficiency results --------\n",
        "print(\"\\nSample efficiency (first episode where sliding-window avg > {} over window={}):\".format(threshold, window))\n",
        "for k, v in sample_efficiency.items():\n",
        "    mean_ep = v['mean_first_ep']\n",
        "    std_ep = v['std_first_ep']\n",
        "    if mean_ep is None:\n",
        "        print(f\"  {k}: never exceeded threshold in any seed. per-seed: {v['all_first']}\")\n",
        "    else:\n",
        "        print(f\"  {k}: first episode ≈ {mean_ep:.1f} ± {std_ep:.1f} (per-seed: {v['all_first']})\")\n",
        "\n",
        "# -------- Optional: visualize greedy policy heatmap for a method (uses last Q of last seed) --------\n",
        "# CliffWalking is 4 x 12 grid (rows x cols), states numbered row-major\n",
        "def plot_policy_arrows(Q, title=\"Greedy policy (arrows)\"):\n",
        "    n_states, n_actions = Q.shape\n",
        "    n_rows, n_cols = 4, 12\n",
        "    assert n_states == n_rows * n_cols\n",
        "    # action mapping (CliffWalking): 0=UP,1=RIGHT,2=DOWN,3=LEFT\n",
        "    action_symbols = {0: '^', 1: '>', 2: 'v', 3: '<'}\n",
        "    policy = np.argmax(Q, axis=1)\n",
        "    fig, ax = plt.subplots(figsize=(12,4))\n",
        "    ax.set_xlim(0, n_cols)\n",
        "    ax.set_ylim(0, n_rows)\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_xticks(np.arange(0,n_cols))\n",
        "    ax.set_yticks(np.arange(0,n_rows))\n",
        "    ax.grid(True)\n",
        "    for s in range(n_states):\n",
        "        r = s // n_cols\n",
        "        c = s % n_cols\n",
        "        a = int(policy[s])\n",
        "        ax.text(c+0.5, r+0.5, action_symbols[a], ha='center', va='center', fontsize=14)\n",
        "    ax.set_title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Pick final Q from each method's last seed and plot\n",
        "for k, qlist in policies.items():\n",
        "    Q_last = qlist[-1]  # last seed's final Q\n",
        "    plot_policy_arrows(Q_last, title=f\"Greedy policy — {k}\")\n",
        "\n",
        "# -------- End of script --------\n"
      ],
      "metadata": {
        "id": "BhimOYaY0zZ7",
        "outputId": "efb29cdf-90cc-4b50-d1dc-0e6aedc2fc0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running SARSA(1) across seeds...\n",
            "Running SARSA TD(5) across seeds...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OverflowError",
          "evalue": "cannot convert float infinity to integer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2759275974.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SARSA_TD5'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_sarsa_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SARSA_TD5'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mpolicies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SARSA_TD5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2759275974.py\u001b[0m in \u001b[0;36mrun_sarsa_n\u001b[0;34m(env_name, seed, n_episodes, alpha, gamma, epsilon, n_step)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# ensure we don’t pass inf to range()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mG\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mr_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtau\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_step\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOverflowError\u001b[0m: cannot convert float infinity to integer"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Solve FrozenLake with Q-Learning and Dyna-Q (Stochastic Model)\n",
        "\n",
        "**Objective**  \n",
        "Implement and compare **Q-learning** and **Dyna-Q** on Gymnasium’s `FrozenLake-v1`.  \n",
        "For Dyna-Q, your learned **transition model must handle multiple possible next states** per `(s, a)` (stochastic slip), i.e., store and sample **a distribution** over `(s', r)` outcomes rather than a single next state.\n",
        "\n",
        "---\n",
        "\n",
        "### Environment\n",
        "- Use `FrozenLake-v1` from `gymnasium.envs.toy_text`.\n",
        "- You can start with map 4×4; and then work with 8×8.\n",
        "- Start → Goal with slippery transitions (stochastic).  \n",
        "- Rewards: `+1` at goal, `0` otherwise (holes terminate with 0).\n",
        "\n",
        "---\n",
        "\n",
        "### Part A — Q-learning (baseline)\n",
        "1. Maintain a tabular action-value function `Q[s, a]`.\n",
        "2. Behavior: ε-greedy over `Q`.\n",
        "3. Update after each real step:\n",
        "   - target = r + γ * max_a' Q[s', a']   (if terminal: target = r)\n",
        "   - Q[s, a] ← Q[s, a] + α * (target − Q[s, a])\n",
        "4. Train for several thousand episodes (e.g., 5,000) with an ε schedule (e.g., 0.2 → 0.01).\n",
        "\n",
        "---\n",
        "\n",
        "### Part B — Dyna-Q with a **stochastic transition model**\n",
        "1. **Empirical model (multinomial):** for each `(s, a)`, maintain a multiset of observed outcomes:\n",
        "   - `model[(s, a)] = [(s'_1, r_1, count_1), (s'_2, r_2, count_2), ...]`\n",
        "   - Update counts whenever you observe `(s, a, r, s')`.\n",
        "2. **Real step update (Q-learning):** same as Part A.\n",
        "3. **Planning steps (N per real step):**\n",
        "   - Sample a previously seen `(s_p, a_p)` uniformly (or with priority).\n",
        "   - Sample `(s'_p, r_p)` **from the empirical distribution** for `(s_p, a_p)` using counts as probabilities.\n",
        "   - Apply the same Q-learning backup with `(s_p, a_p, r_p, s'_p)`.\n",
        "4. Train with the same ε schedule and number of episodes; vary `N ∈ {5, 20, 50}`.\n",
        "\n",
        "---\n",
        "\n",
        "### Experiments & Analysis\n",
        "1. **Learning curves:** plot episode index vs episode return (smoothed) for:\n",
        "   - Q-learning\n",
        "   - Dyna-Q (N=5, 20, 50)\n",
        "2. **Sample efficiency:** report the episode at which the moving-average return (e.g., window 100) first exceeds a threshold (you choose a reasonable value).\n",
        "3. **Effect of stochastic modeling:** briefly explain why storing a distribution over `(s', r)` matters on FrozenLake (slip), and what happens if you store only the most recent outcome.\n",
        "4. **Robustness:** run ≥5 random seeds; report mean ± std of final evaluation returns.\n",
        "\n",
        "---\n",
        "\n",
        "### Deliverables\n",
        "- **Code** for Q-learning and Dyna-Q (with stochastic model).  \n",
        "- **Plots** of learning curves (include legend and axis labels).  \n",
        "- ** Discussion:** why Dyna-Q helps here; impact of N; importance of modeling multiple next states.\n",
        "\n",
        "---\n",
        "\n",
        "### Hints\n",
        "- For terminal transitions (goal/hole), the Q-learning target is simply `target = r` (no bootstrap).  \n",
        "- When sampling from the model, use probabilities `p_i = count_i / sum_j count_j`.  \n",
        "- Tie-break greedy action selection uniformly among argmax actions to avoid bias.  \n",
        "- Keep evaluation **greedy (ε=0)** and consistent across methods (same seeds and episode counts).\n"
      ],
      "metadata": {
        "id": "E4iLQFaGzJLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FrozenLake: Q-Learning vs Dyna-Q (stochastic model)\n",
        "# Requires: gymnasium, numpy, matplotlib\n",
        "# Place in a Jupyter cell and run.\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# -------------------------\n",
        "# Experiment configuration\n",
        "# -------------------------\n",
        "map_name = \"4x4\"              # \"4x4\" or \"8x8\"\n",
        "is_slippery = True            # stochastic transitions (slip)\n",
        "env_name = \"FrozenLake-v1\"\n",
        "\n",
        "gamma = 0.99\n",
        "alpha = 0.1\n",
        "eps_start = 0.2\n",
        "eps_end = 0.01\n",
        "n_episodes = 5000            # per seed\n",
        "seeds = [0,1,2,3,4]          # >=5 seeds for robustness\n",
        "window = 100                 # moving average window for sample-efficiency\n",
        "# thresholds: choose reasonable based on map size\n",
        "threshold_by_map = {\"4x4\": 0.78, \"8x8\": 0.4}\n",
        "threshold = threshold_by_map.get(map_name, 0.5)\n",
        "\n",
        "dyna_N_list = [5, 20, 50]\n",
        "\n",
        "# -------------------------\n",
        "# Helpers\n",
        "# -------------------------\n",
        "def make_env(seed=None):\n",
        "    # gymnasium FrozenLake-v1\n",
        "    return gym.make(env_name, map_name=map_name, is_slippery=is_slippery)\n",
        "\n",
        "def tie_break_argmax(q_values):\n",
        "    \"\"\"Return an argmax index breaking ties uniformly.\"\"\"\n",
        "    top = np.flatnonzero(q_values == np.max(q_values))\n",
        "    return int(np.random.choice(top))\n",
        "\n",
        "def epsilon_by_episode(ep, total_eps=n_episodes, start=eps_start, end=eps_end):\n",
        "    \"\"\"Linear decay from start to end across episodes.\"\"\"\n",
        "    frac = min(1.0, ep / float(total_eps))\n",
        "    return start + frac * (end - start)\n",
        "\n",
        "# -------------------------\n",
        "# Q-Learning implementation\n",
        "# -------------------------\n",
        "def run_q_learning(seed, n_episodes=n_episodes):\n",
        "    env = make_env(seed)\n",
        "    env.reset(seed=seed)\n",
        "    n_states = env.observation_space.n\n",
        "    n_actions = env.action_space.n\n",
        "    Q = np.zeros((n_states, n_actions))\n",
        "    returns = np.zeros(n_episodes)\n",
        "    for ep in range(n_episodes):\n",
        "        eps = epsilon_by_episode(ep)\n",
        "        obs, _ = env.reset(seed=(seed + ep))\n",
        "        done = False\n",
        "        ep_ret = 0.0\n",
        "        while True:\n",
        "            # epsilon-greedy with tie-breaking\n",
        "            if np.random.rand() < eps:\n",
        "                a = np.random.randint(n_actions)\n",
        "            else:\n",
        "                a = tie_break_argmax(Q[obs])\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            ep_ret += reward\n",
        "\n",
        "            if done:\n",
        "                target = reward\n",
        "            else:\n",
        "                target = reward + gamma * np.max(Q[next_obs])\n",
        "            Q[obs, a] += alpha * (target - Q[obs, a])\n",
        "\n",
        "            obs = next_obs\n",
        "            if done:\n",
        "                break\n",
        "        returns[ep] = ep_ret\n",
        "    env.close()\n",
        "    return returns, Q\n",
        "\n",
        "# -------------------------\n",
        "# Dyna-Q with stochastic multinomial model\n",
        "# -------------------------\n",
        "class StochasticModel:\n",
        "    \"\"\"Empirical model storing counts of observed (s' , r) outcomes per (s,a).\"\"\"\n",
        "    def __init__(self):\n",
        "        # model[(s,a)] -> dict mapping s' -> count (rewards deterministic for FrozenLake: 1 at goal else 0)\n",
        "        self.counts = defaultdict(lambda: defaultdict(int))\n",
        "        self.rewards = {}  # optional map from s' to r - reward depends only on s' in FrozenLake\n",
        "        self.sa_seen = set()\n",
        "\n",
        "    def update(self, s, a, s_next, r):\n",
        "        self.counts[(s,a)][s_next] += 1\n",
        "        self.rewards[s_next] = r\n",
        "        self.sa_seen.add((s,a))\n",
        "\n",
        "    def sample(self, s, a):\n",
        "        \"\"\"Sample (s', r) according to observed empirical distribution for (s,a).\"\"\"\n",
        "        outcomes = self.counts.get((s,a), None)\n",
        "        if outcomes is None or len(outcomes) == 0:\n",
        "            # unseen (s,a) -> no-op: return None\n",
        "            return None\n",
        "        states = list(outcomes.keys())\n",
        "        counts = np.array([outcomes[s2] for s2 in states], dtype=float)\n",
        "        probs = counts / counts.sum()\n",
        "        s_next = np.random.choice(states, p=probs)\n",
        "        r = self.rewards[s_next]\n",
        "        return s_next, r\n",
        "\n",
        "    def get_seen_pairs(self):\n",
        "        return list(self.sa_seen)\n",
        "\n",
        "def run_dyna_q(seed, planning_steps=5, n_episodes=n_episodes):\n",
        "    env = make_env(seed)\n",
        "    env.reset(seed=seed)\n",
        "    n_states = env.observation_space.n\n",
        "    n_actions = env.action_space.n\n",
        "    Q = np.zeros((n_states, n_actions))\n",
        "    model = StochasticModel()\n",
        "    returns = np.zeros(n_episodes)\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        eps = epsilon_by_episode(ep)\n",
        "        obs, _ = env.reset(seed=(seed + ep))\n",
        "        ep_ret = 0.0\n",
        "        while True:\n",
        "            # epsilon-greedy policy with tie-breaking\n",
        "            if np.random.rand() < eps:\n",
        "                a = np.random.randint(n_actions)\n",
        "            else:\n",
        "                a = tie_break_argmax(Q[obs])\n",
        "\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            ep_ret += reward\n",
        "\n",
        "            # real Q-learning update\n",
        "            if done:\n",
        "                target = reward\n",
        "            else:\n",
        "                target = reward + gamma * np.max(Q[next_obs])\n",
        "            Q[obs, a] += alpha * (target - Q[obs, a])\n",
        "\n",
        "            # update model (store observed s' counts)\n",
        "            model.update(obs, a, next_obs, reward)\n",
        "\n",
        "            # planning: sample prior (s_p,a_p), then sample (s'_p,r_p) from model\n",
        "            seen_pairs = model.get_seen_pairs()\n",
        "            for _ in range(planning_steps):\n",
        "                if len(seen_pairs) == 0:\n",
        "                    break\n",
        "                s_p, a_p = random.choice(seen_pairs)\n",
        "                sampled = model.sample(s_p, a_p)\n",
        "                if sampled is None:\n",
        "                    continue\n",
        "                s_next_p, r_p = sampled\n",
        "                # simulated Q-learning update\n",
        "                # if s_next_p terminal: reward is r_p and no bootstrap; handled since reward at terminal is r_p\n",
        "                target_p = r_p + gamma * np.max(Q[s_next_p])\n",
        "                Q[s_p, a_p] += alpha * (target_p - Q[s_p, a_p])\n",
        "\n",
        "            obs = next_obs\n",
        "            if done:\n",
        "                break\n",
        "        returns[ep] = ep_ret\n",
        "    env.close()\n",
        "    return returns, Q\n",
        "\n",
        "# -------------------------\n",
        "# Utilities: aggregate, moving average, sample-efficiency\n",
        "# -------------------------\n",
        "def aggregate_runs(runs):\n",
        "    arr = np.vstack(runs)  # shape (n_seeds, n_episodes)\n",
        "    mean = arr.mean(axis=0)\n",
        "    std = arr.std(axis=0)\n",
        "    return mean, std\n",
        "\n",
        "def moving_average(x, w=window):\n",
        "    if len(x) < w:\n",
        "        return np.array([])\n",
        "    return np.convolve(x, np.ones(w)/w, mode='valid')\n",
        "\n",
        "def first_episode_exceeding(runs, w, threshold):\n",
        "    # runs: list of 1d arrays\n",
        "    firsts = []\n",
        "    for r in runs:\n",
        "        ma = moving_average(r, w)\n",
        "        idxs = np.where(ma >= threshold)[0]\n",
        "        if len(idxs) == 0:\n",
        "            firsts.append(None)\n",
        "        else:\n",
        "            # convert moving-average index to episode index (use the last episode of the window)\n",
        "            firsts.append(int(idxs[0] + w - 1))\n",
        "    valid = [x for x in firsts if x is not None]\n",
        "    if len(valid) == 0:\n",
        "        return None, None, firsts\n",
        "    return np.mean(valid), np.std(valid), firsts\n",
        "\n",
        "# -------------------------\n",
        "# Run experiments\n",
        "# -------------------------\n",
        "methods = {}\n",
        "# Q-learning across seeds\n",
        "print(\"Running Q-learning...\")\n",
        "ql_runs = []\n",
        "ql_Qs = []\n",
        "for seed in seeds:\n",
        "    r, Qfinal = run_q_learning(seed)\n",
        "    ql_runs.append(r)\n",
        "    ql_Qs.append(Qfinal)\n",
        "methods['Q-Learning'] = (ql_runs, ql_Qs)\n",
        "\n",
        "# Dyna-Q with different N\n",
        "for N in dyna_N_list:\n",
        "    key = f\"DynaQ_N{N}\"\n",
        "    print(f\"Running {key} ...\")\n",
        "    runs = []\n",
        "    Qs = []\n",
        "    for seed in seeds:\n",
        "        r, Qfinal = run_dyna_q(seed, planning_steps=N)\n",
        "        runs.append(r)\n",
        "        Qs.append(Qfinal)\n",
        "    methods[key] = (runs, Qs)\n",
        "\n",
        "# -------------------------\n",
        "# Aggregate & Plot\n",
        "# -------------------------\n",
        "plt.figure(figsize=(12, 8))\n",
        "aggregated = {}\n",
        "for k, (runs, Qs) in methods.items():\n",
        "    mean, std = aggregate_runs(runs)\n",
        "    aggregated[k] = (mean, std)\n",
        "    episodes = np.arange(1, len(mean)+1)\n",
        "    plt.plot(episodes, moving_average(mean, w=1), label=k)  # raw mean (thin)\n",
        "    # plot smoothed mean (window)\n",
        "    sm = moving_average(mean, w=window)\n",
        "    ep_sm = np.arange(window, len(mean)+1)\n",
        "    plt.plot(ep_sm, sm, linewidth=2)\n",
        "    plt.fill_between(ep_sm, sm - moving_average(std, w=window), sm + moving_average(std, w=window), alpha=0.2)\n",
        "\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Episode Return (0 or 1)\")\n",
        "plt.title(f\"Q-Learning vs Dyna-Q (stochastic model) on FrozenLake {map_name} (slippery={is_slippery})\\nSmoothed mean ± std (window={window})\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# -------------------------\n",
        "# Sample efficiency reporting\n",
        "# -------------------------\n",
        "print(\"\\nSample-efficiency (first episode when moving-average (w={}) >= {:.3f}):\".format(window, threshold))\n",
        "for k, (runs, Qs) in methods.items():\n",
        "    mean_ep, std_ep, per_seed = first_episode_exceeding(runs, window, threshold)\n",
        "    if mean_ep is None:\n",
        "        print(f\"  {k}: never exceeded threshold. per-seed: {per_seed}\")\n",
        "    else:\n",
        "        print(f\"  {k}: first episode ≈ {mean_ep:.1f} ± {std_ep:.1f}  (per-seed: {per_seed})\")\n",
        "\n",
        "# -------------------------\n",
        "# Final greedy evaluation (ε=0) after training: run 100 evaluation episodes per seed\n",
        "# -------------------------\n",
        "def evaluate_greedy(Q, seed, n_eval=100):\n",
        "    env = make_env(seed)\n",
        "    env.reset(seed=seed)\n",
        "    total = 0.0\n",
        "    for ep in range(n_eval):\n",
        "        obs, _ = env.reset(seed=(seed+ep))\n",
        "        done = False\n",
        "        while True:\n",
        "            a = tie_break_argmax(Q[obs])\n",
        "            obs, r, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            if done:\n",
        "                total += r\n",
        "                break\n",
        "    env.close()\n",
        "    return total / n_eval\n",
        "\n",
        "print(\"\\nFinal greedy evaluation (mean success rate over seeds):\")\n",
        "for k, (runs, Qs) in methods.items():\n",
        "    evals = []\n",
        "    for i, Q in enumerate(Qs):\n",
        "        evals.append(evaluate_greedy(Q, seed=seeds[i], n_eval=200))\n",
        "    print(f\"  {k}: mean={np.mean(evals):.3f} ± std={np.std(evals):.3f} (per-seed evals: {np.round(evals,3)})\")\n"
      ],
      "metadata": {
        "id": "d7FHlfk700lr",
        "outputId": "7cf5a13c-334a-4b03-e995-64f22d7ed54b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Q-learning...\n",
            "Running DynaQ_N5 ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1819866053.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mQs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_dyna_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplanning_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0mruns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mQs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1819866053.py\u001b[0m in \u001b[0;36mrun_dyna_q\u001b[0;34m(seed, planning_steps, n_episodes)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtie_break_argmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1819866053.py\u001b[0m in \u001b[0;36mtie_break_argmax\u001b[0;34m(q_values)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m\"\"\"Return an argmax index breaking ties uniformly.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mepsilon_by_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_eps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mnumpy/random/_bounded_integers.pyx\u001b[0m in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_prod_dispatcher\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3063\u001b[0;31m def _prod_dispatcher(a, axis=None, dtype=None, out=None, keepdims=None,\n\u001b[0m\u001b[1;32m   3064\u001b[0m                      initial=None, where=None):\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}