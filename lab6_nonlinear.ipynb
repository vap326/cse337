{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vap326/cse337/blob/main/lab6_nonlinear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 6: Non-linear function approximation\n",
        "\n",
        "## Exercise 1: Q-Learning with a Neural Network (PyTorch) on MountainCar\n",
        "\n",
        "**Objective:**\n",
        "Implement **Q-learning** with a **PyTorch neural network** to solve `MountainCar-v0`. You will approximate Q(s, a) with a small MLP, train it from batches of transitions sampled from a replay buffer, and evaluate the learned policy.\n",
        "\n",
        "---\n",
        "\n",
        "## Environment\n",
        "- **Gym** environment: `MountainCar-v0`\n",
        "- **State**: continuous (position, velocity) → shape `(2,)`\n",
        "- **Actions**: {0: left, 1: no push, 2: right}\n",
        "- **Reward**: -1 per step until the goal (`position >= 0.5`)\n",
        "- **Episode limit**: 500 steps\n",
        "- **Goal**: reduce steps-to-goal and improve return over training\n",
        "\n",
        "---\n",
        "\n",
        "## What You Must Implement\n",
        "\n",
        "### 1) Q-Network (PyTorch)\n",
        "Create a small MLP `QNetwork` that maps `state -> Q-values for 3 actions`.\n",
        "- Inputs: `(batch_size, 2)` float32\n",
        "- Outputs: `(batch_size, 3)` Q-values\n",
        "- Suggested architecture: `2 → 64 → 3` with ReLU\n",
        "- Initialize weights reasonably (PyTorch defaults are fine)\n",
        "\n",
        "### 2) Replay Buffer\n",
        "A cyclic buffer to store transitions `(s, a, r, s_next, done)`:\n",
        "- `append(s, a, r, s_next, done)`\n",
        "- `sample(batch_size)` → tensors ready for PyTorch (float32 for states, int64 for actions, float32 for rewards/done)\n",
        "\n",
        "### 3) ε-Greedy Policy\n",
        "- With probability `epsilon`: pick a random action\n",
        "- Otherwise: `argmax_a Q(s, a)` from the current network\n",
        "- Use **decaying ε** (e.g., from 1.0 down to 0.05 over ~20–50k steps)\n",
        "\n",
        "### 4) Q-Learning Target and Loss\n",
        "For a sampled batch:\n",
        "- Compute `q_pred = Q(s).gather(1, a)`  (shape `(batch, 1)`)\n",
        "- Compute target:\n",
        "  - If `done`: `target = r`\n",
        "  - Else: `target = r + gamma * max_a' Q(s_next, a').detach()`\n",
        "- Loss: Mean Squared Error (MSE) between `q_pred` and `target`\n",
        "\n",
        "> **Stabilization (recommended)**: Use a **target network** `Q_target` (periodically copy weights from `Q_online`) to compute the max over next-state actions. Update every `target_update_freq` steps.\n",
        "\n",
        "### 5) Deep Q-learning method\n",
        "- For each environment step:\n",
        "  1. Select action with ε-greedy\n",
        "  2. Step the env, store transition in buffer\n",
        "  3. If `len(buffer) >= batch_size`:\n",
        "     - Sample a batch\n",
        "     - Compute `q_pred`, `target`\n",
        "     - Backprop: `optimizer.zero_grad(); loss.backward(); optimizer.step()`\n",
        "     - (Optional) gradient clipping (e.g., `clip_grad_norm_` at 10)\n",
        "  4. Periodically update `Q_target ← Q_online` (if using target net)\n",
        "- Track episode returns (sum of rewards) and steps-to-goal\n",
        "\n",
        "---\n",
        "\n",
        "## Evaluation\n",
        "- Run **evaluation episodes** with `epsilon = 0.0` (greedy) every N training episodes\n",
        "- Report:\n",
        "  - Average steps-to-goal (lower is better; random policy is ~200)\n",
        "  - Average return (less negative is better)\n",
        "- Plot:\n",
        "  - Training episode return\n",
        "\n",
        "---\n",
        "\n",
        "## Deliverables\n",
        "1. **Code**: In a notebook.\n",
        "2. **Plots**:\n",
        "   - Episode  vs return\n",
        "   - Final value function (State (postition and velocity) Vs Max(Q(state)))\n",
        "\n",
        "3. **Short write-up** (also in the notebook):\n",
        "   - **Performance of your DQN agent**: How quickly does it learn? Does it reach the goal consistently?\n",
        "   - **Comparison with tile coding**:\n",
        "     - Which representation learns faster?\n",
        "     - Which one is more stable?\n",
        "     - How do the function approximation choices (linear with tiles vs. neural network) affect generalization?\n",
        "     - Did the NN require more tuning (learning rate, ε schedule) compared to tile coding?\n",
        "   - **Insights**: What are the trade-offs between hand-crafted features (tiles) and learned features (neural networks)?\n",
        "\n"
      ],
      "metadata": {
        "id": "DzEu8zQt3_MJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DHYJhmAv355q"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set up environment\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "alpha = 0.001\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "num_episodes = 5000\n",
        "batch_size = 64\n",
        "replay_buffer_size = 50000\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Q-Network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, n_actions):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "JhoDESZd60Yu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Q-network and optimizer\n",
        "q_net = QNetwork(state_dim, n_actions).to(device)\n",
        "optimizer = optim.Adam(q_net.parameters(), lr=alpha)\n",
        "loss_fn = nn.MSELoss()\n",
        "replay_buffer = deque(maxlen=replay_buffer_size)"
      ],
      "metadata": {
        "id": "erbbkUXL65HM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy(state, epsilon):\n",
        "  ############ TODO ###########\n",
        "  if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_actions)\n",
        "  state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "  q_values = q_net(state_t)\n",
        "  return q_values.argmax().item()\n",
        "  ## epsilon decay is added outside for loop, in main"
      ],
      "metadata": {
        "id": "nk6hESNp7KMk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dqn():\n",
        "    \"\"\"Train the DQN using experience replay.\"\"\"\n",
        "    if len(replay_buffer) < batch_size:\n",
        "        return\n",
        "    batch = random.sample(replay_buffer, batch_size)\n",
        "    states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "    states = torch.FloatTensor(states).to(device)\n",
        "    actions = torch.LongTensor(actions).to(device)\n",
        "    rewards = torch.FloatTensor(rewards).to(device)\n",
        "    next_states = torch.FloatTensor(next_states).to(device)\n",
        "    dones = torch.FloatTensor(dones).to(device)\n",
        "\n",
        "    q_values = q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "    next_q_values = q_net(next_states).max(1)[0].detach()\n",
        "    targets = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "    loss = loss_fn(q_values, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "hcX56dEL7PZd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## MAIN Loop ###\n",
        "rewards_dqn = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "  state = env.reset()[0]\n",
        "  total_reward = 0\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    action = epsilon_greedy(state, epsilon)\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "    ############ TODO ###########\n",
        "    replay_buffer.append((state, action, reward, next_state, done))\n",
        "    train_dqn()\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "  epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "  rewards_dqn.append(total_reward)\n",
        "\n",
        "  if (episode + 1) % 100 == 0:\n",
        "      avg_reward = np.mean(rewards_dqn[-100:])\n",
        "      print(f\"Episode {episode+1}/{num_episodes}, \"\n",
        "      f\"Epsilon: {epsilon:.3f}, Avg Reward (last 100): {avg_reward:.2f}\")\n",
        "\n",
        "\n",
        "plt.plot(rewards_dqn)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"Training Progress - DQN on MountainCar-v0\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kgR0ojdN7RuM",
        "outputId": "594dccca-afc8-473d-ff15-08d133787424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-314177893.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m############ TODO ###########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1929135722.py\u001b[0m in \u001b[0;36mtrain_dqn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m                             )\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    245\u001b[0m             )\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    420\u001b[0m                     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mexp_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2: Deep Q-Learning (DQN) on LunarLander-v2\n",
        "\n",
        "## Problem Description\n",
        "In this exercise, you will implement **Deep Q-Learning (DQN)** to solve the classic control problem **LunarLander-v2** in Gym.\n",
        "\n",
        "### The Task\n",
        "The agent controls a lander that starts at the top of the screen and must safely land on the landing pad between two flags.\n",
        "\n",
        "- **State space**: Continuous vector of 8 variables, including:\n",
        "  - Position (x, y)\n",
        "  - Velocity (x_dot, y_dot)\n",
        "  - Angle and angular velocity\n",
        "  - Left/right leg contact indicators\n",
        "- **Action space**: Discrete, 4 actions\n",
        "  - 0: do nothing\n",
        "  - 1: fire left orientation engine\n",
        "  - 2: fire main engine\n",
        "  - 3: fire right orientation engine\n",
        "- **Rewards**:\n",
        "  - +100 to +140 for successful landing\n",
        "  - -100 for crashing\n",
        "  - Small negative reward for firing engines (fuel cost)\n",
        "  - Episode ends when lander crashes or comes to rest\n",
        "\n",
        "The goal is to train an agent that lands successfully **most of the time**.\n",
        "\n",
        "---\n",
        "\n",
        "## Algorithm: Deep Q-Learning\n",
        "You will implement a **DQN agent** with the following components:\n",
        "\n",
        "1. **Q-Network**\n",
        "   - Neural network that approximates Q(s, a).\n",
        "   - Input: state vector (8 floats).\n",
        "   - Output: Q-values for 4 actions.\n",
        "   - Suggested architecture: 2 hidden layers with 128 neurons each, ReLU activation.\n",
        "\n",
        "2. **Target Network**\n",
        "   - A copy of the Q-network that is updated less frequently (e.g., every 1000 steps).\n",
        "   - Used for stable target computation.\n",
        "\n",
        "3. **Replay Buffer**\n",
        "   - Stores transitions `(s, a, r, s_next, done)`.\n",
        "   - Sample random mini-batches to break correlation between consecutive samples.\n",
        "\n",
        "4. **ε-Greedy Policy**\n",
        "   - With probability ε, take a random action.\n",
        "   - Otherwise, take `argmax_a Q(s, a)`.\n",
        "   - Decay ε over time (e.g., from 1.0 → 0.05).\n",
        "\n",
        "5. **Q-Learning Method**\n",
        "   \n",
        "\n",
        "\n",
        "**Final note:**\n",
        "   No code base is necessary. At this point, you must know how to implement evertything.\n",
        "   For reference, but not recommended ([Here](https://colab.research.google.com/drive/1Gl0kuln79A__hgf2a-_-mwoGISXQDK_X?authuser=1#scrollTo=8Sd0q9DG8Rt8&line=56&uniqifier=1) is a solution)\n",
        "\n",
        "---\n",
        "## Deliverables\n",
        "1. **Code**:\n",
        "- Q-network (PyTorch).\n",
        "- Training loop with ε-greedy policy, target network, and Adam optimizer.\n",
        "\n",
        "2. **Plots**:\n",
        "- Episode returns vs training episodes.\n",
        "- Evaluation performance with a greedy policy (ε = 0).\n",
        "\n",
        "3. **Short Write-up (≤1 page)**:\n",
        "- Did your agent learn to land consistently?  \n",
        "- How many episodes did it take before you saw improvement?  \n",
        "- What effect did replay buffer size, target update frequency, and learning rate have on stability?  \n",
        "- Compare results across different runs (does it sometimes fail to converge?).\n",
        "\n",
        "Compare this task with the **MountainCar-v0** problem you solved earlier:\n",
        "- What is **extra** or more challenging in LunarLander?  \n",
        "- Consider state dimensionality, number of actions, reward shaping, and the difficulty of exploration.  \n",
        "- Why might DQN be necessary here, whereas simpler methods (like tile coding) could work for MountainCar?\n"
      ],
      "metadata": {
        "id": "8Sd0q9DG8Rt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set up environment\n",
        "env = gym.make(\"LunarLander-v3\")\n",
        "state_dim = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "alpha = 0.001\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "num_episodes = 1000\n",
        "batch_size = 64\n",
        "replay_buffer_size = 50000\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, n_actions):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "q_net = QNetwork(state_dim, n_actions).to(device)\n",
        "target_net = QNetwork(state_dim, n_actions).to(device)\n",
        "target_net.load_state_dict(q_net.state_dict())\n",
        "optimizer = optim.Adam(q_net.parameters(), lr=alpha)\n",
        "loss_fn = nn.MSELoss()\n",
        "replay_buffer = deque(maxlen=replay_buffer_size)\n",
        "\n",
        "def epsilon_greedy(state, epsilon):\n",
        "  ############ TODO ###########\n",
        "  if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_actions)\n",
        "  state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "  q_values = q_net(state_t)\n",
        "  return q_values.argmax().item()\n",
        "\n",
        "def train_dqn():\n",
        "    \"\"\"Train the DQN using experience replay.\"\"\"\n",
        "    if len(replay_buffer) < batch_size:\n",
        "        return\n",
        "    batch = random.sample(replay_buffer, batch_size)\n",
        "    states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "    states = torch.FloatTensor(states).to(device)\n",
        "    actions = torch.LongTensor(actions).to(device)\n",
        "    rewards = torch.FloatTensor(rewards).to(device)\n",
        "    next_states = torch.FloatTensor(next_states).to(device)\n",
        "    dones = torch.FloatTensor(dones).to(device)\n",
        "\n",
        "    q_values = q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "    next_q_values = q_net(next_states).max(1)[0].detach()\n",
        "    targets = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "    loss = loss_fn(q_values, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "## MAIN Loop ###\n",
        "rewards_dqn = []\n",
        "total_steps = 0\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "  state = env.reset()[0]\n",
        "  total_reward = 0\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    action = epsilon_greedy(state, epsilon)\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "    ############ TODO ###########\n",
        "    replay_buffer.append((state, action, reward, next_state, done))\n",
        "    train_dqn()\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "    total_steps += 1\n",
        "  if total_steps % 1000 == 0:\n",
        "            target_net.load_state_dict(q_net.state_dict())\n",
        "  epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "  rewards_dqn.append(total_reward)\n",
        "\n",
        "\n",
        "if (episode + 1) % 100 == 0:\n",
        "    avg_reward = np.mean(rewards_dqn[-100:])\n",
        "    print(f\"Episode {episode+1}/{num_episodes}, \"\n",
        "    f\"Epsilon: {epsilon:.3f}, Avg Reward (last 100): {avg_reward:.2f}\")\n",
        "\n",
        "\n",
        "plt.plot(rewards_dqn)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"Training Progress - DQN on MountainCar-v0\")\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "J6IXyZqR7zia",
        "outputId": "da51a86e-1ecd-4aaa-dd5e-3d77a35eb279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DependencyNotInstalled",
          "evalue": "Box2D is not installed, you can install it by run `pip install swig` followed by `pip install \"gymnasium[box2d]\"`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mBox2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     from Box2D.b2 import (\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3162101112.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Set up environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LunarLander-v3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mstate_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mn_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;31m# Assume it's a string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m         \u001b[0menv_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_env_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;31m# Determine if to use the rendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36mload_env_creator\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \"\"\"\n\u001b[1;32m    543\u001b[0m     \u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/box2d/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgymnasium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbipedal_walker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBipedalWalker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBipedalWalkerHardcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgymnasium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcar_racing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCarRacing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgymnasium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlunar_lander\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLunarLander\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLunarLanderContinuous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     raise DependencyNotInstalled(\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;34m'Box2D is not installed, you can install it by run `pip install swig` followed by `pip install \"gymnasium[box2d]\"`'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     ) from e\n",
            "\u001b[0;31mDependencyNotInstalled\u001b[0m: Box2D is not installed, you can install it by run `pip install swig` followed by `pip install \"gymnasium[box2d]\"`"
          ]
        }
      ]
    }
  ]
}